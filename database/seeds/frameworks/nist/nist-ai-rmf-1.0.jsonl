{"page":1,"tags":["Unnamed Node","Preamble","Unnamed Node"],"text":"The GOVERN function:\n\u2022\tcultivates and implements a culture of risk management within organizations designing, developing, deploying, evaluating, or acquiring AI systems;\n\u2022\toutlines processes, documents, and organizational schemes that anticipate, identify, and manage the risks a system can pose, including to users and others across society \u2013 and procedures to achieve those outcomes;\n\u2022\tincorporates processes to assess potential impacts;\n\u2022\tprovides a structure by which AI risk management functions can align with organizational principles, policies, and strategic priorities;\n\u2022\tconnects technical aspects of AI system design and development to organizational values and principles, and enables organizational practices and competencies for the individuals involved in acquiring, training, deploying, and monitoring such systems; and\n\u2022\taddresses full product lifecycle and associated processes, including legal and other issues concerning use of third-party software or hardware systems and data."}
{"page":1,"tags":["Unnamed Node","Preamble","Unnamed Node"],"text":"GOVERN is a cross-cutting function that is infused throughout AI risk management and enables the other functions of the process. Aspects of GOVERN, especially those related to compliance or evaluation, should be integrated into each of the other functions. Attention to governance is a continual and intrinsic requirement for effective AI risk management over an AI system\u2019s lifespan and the organization\u2019s hierarchy."}
{"page":1,"tags":["Unnamed Node","Preamble","Unnamed Node"],"text":"Strong governance can drive and enhance internal practices and norms to facilitate organizational risk culture. Governing authorities can determine the overarching policies that direct an organization\u2019s mission, goals, values, culture, and risk tolerance. Senior leadership sets the tone for risk management within an organization, and with it, organizational culture. Management aligns the technical aspects of AI risk management to policies and operations. Documentation can enhance transparency, improve human review processes, and bolster accountability in AI system teams."}
{"page":1,"tags":["Unnamed Node","Preamble","Unnamed Node"],"text":"After putting in place the structures, systems, processes, and teams described in the GOVERN function, organizations should benefit from a purpose-driven culture focused on risk understanding and management. It is incumbent on Framework users to continue to execute the GOVERN function as knowledge, cultures, and needs or expectations from AI actors evolve over time."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Legal and regulatory requirements involving AI are understood, managed, and documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"The characteristics of trustworthy AI are integrated into organizational policies, processes, procedures, and practices."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Processes, procedures, and practices are in place to determine the needed level of risk management activities based on the organization\u2019s risk tolerance."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"The risk management process and its outcomes are established through transparent policies, procedures, and other controls based on organizational risk priorities."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Ongoing monitoring and periodic review of the risk management process and its outcomes are planned and organizational roles and responsibilities clearly defined, including determining the frequency of periodic review."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Mechanisms are in place to inventory AI systems and are resourced according to organizational risk priorities."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Processes and procedures are in place for decommissioning and phasing out AI systems safely and in a manner that does not increase risks or decrease the organization\u2019s trustworthiness."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Roles and responsibilities and lines of communication related to mapping, measuring, and managing AI risks are documented and are clear to individuals and teams throughout the organization."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"The organization\u2019s personnel and partners receive AI risk management training to enable them to perform their duties and responsibilities consistent with related policies, procedures, and agreements."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Executive leadership of the organization takes responsibility for decisions about risks associated with AI system development and deployment."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Decision-making related to mapping, measuring, and managing AI risks throughout the lifecycle is informed by a diverse team (e.g., diversity of demographics, disciplines, experience, expertise, and backgrounds)."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Policies and procedures are in place to define and differentiate roles and responsibilities for human-AI configurations and oversight of AI systems."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Organizational policies and practices are in place to foster a critical thinking and safety-first mindset in the design, development, deployment, and uses of AI systems to minimize potential negative impacts."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Organizational teams document the risks and potential impacts of the AI technology they design, develop, deploy, evaluate, and use, and they communicate about the impacts more broadly."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Organizational practices are in place to enable AI testing, identification of incidents, and information sharing."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Organizational policies and practices are in place to collect, consider, prioritize, and integrate feedback from those external to the team that developed or deployed the AI system regarding the potential individual and societal impacts related to AI risks."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Mechanisms are established to enable the team that developed or deployed AI systems to regularly incorporate adjudicated feedback from relevant AI actors into system design and implementation."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Policies and procedures are in place that address AI risks associated with third-party entities, including risks of infringement of a third-party\u2019s intellectual property or other rights."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Contingency processes are in place to handle failures or incidents in third-party data or AI systems deemed to be high-risk."}
{"page":1,"tags":["Unnamed Node","Preamble","Unnamed Node"],"text":"The MAP function establishes the context to frame risks related to an AI system. The AI lifecycle consists of many interdependent activities involving a diverse set of actors (See Figure 3). In practice, AI actors in charge of one part of the process often do not have full visibility or control over other parts and their associated contexts. The interdependencies between these activities, and among the relevant AI actors, can make it difficult to reliably anticipate impacts of AI systems. For example, early decisions in identifying purposes and objectives of an AI system can alter its behavior and capabilities, and the dynamics of deployment setting (such as end users or impacted individuals) can shape the impacts of AI system decisions. As a result, the best intentions within one dimension of the AI lifecycle can be undermined via interactions with decisions and conditions in other, later activities.\nThis complexity and varying levels of visibility can introduce uncertainty into risk management practices. Anticipating, assessing, and otherwise addressing potential sources of negative risk can mitigate this uncertainty and enhance the integrity of the decision process."}
{"page":1,"tags":["Unnamed Node","Preamble","Unnamed Node"],"text":"The information gathered while carrying out the MAP function enables negative risk prevention and informs decisions for processes such as model management, as well as an initial decision about appropriateness or the need for an AI solution. Outcomes in the MAP function are the basis for the MEASURE and MANAGE functions. Without contextual knowledge, and awareness of risks within the identified contexts, risk management is difficult to perform. The MAP function is intended to enhance an organization\u2019s ability to identify risks and broader contributing factors."}
{"page":1,"tags":["Unnamed Node","Preamble","Unnamed Node"],"text":"Implementation of this function is enhanced by incorporating perspectives from a diverse internal team and engagement with those external to the team that developed or deployed the AI system. Engagement with external collaborators, end users, potentially impacted communities, and others may vary based on the risk level of a particular AI system, the makeup of the internal team, and organizational policies. Gathering such broad perspectives can help organizations proactively prevent negative risks and develop more trustworthy AI systems by:\n\u2022\timproving their capacity for understanding contexts;\n\u2022\tchecking their assumptions about context of use;\n\u2022\tenabling recognition of when systems are not functional within or out of their intended context;\n\u2022\tidentifying positive and beneficial uses of their existing AI systems;\n\u2022\timproving understanding of limitations in AI and ML processes;\n\u2022\tidentifying constraints in real-world applications that may lead to negative impacts;\n\u2022\tidentifying known and foreseeable negative impacts related to intended use of AI systems; and\n\u2022\tanticipating risks of the use of AI systems beyond intended use."}
{"page":1,"tags":["Unnamed Node","Preamble","Unnamed Node"],"text":"After completing the MAP function, Framework users should have sufficient contextual knowledge about AI system impacts to inform an initial go\/no-go decision about whether to design, develop, or deploy an AI system. If a decision is made to proceed, organizations should utilize the MEASURE and MANAGE functions along with policies and procedures put into place in the GOVERN function to assist in AI risk management efforts. It is incumbent on Framework users to continue applying the MAP function to AI systems as context, capabilities, risks, benefits, and potential impacts evolve over time."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Intended purposes, potentially beneficial uses, contextspecific laws, norms and expectations, and prospective settings in which the AI system will be deployed are understood and documented. Considerations include: the specific set or types of users along with their expectations; potential positive and negative impacts of system uses to individuals, communities, organizations, society, and the planet; assumptions and related limitations about AI system purposes, uses, and risks across the development or product AI lifecycle; and related TEVV and system metrics."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Interdisciplinary AI actors, competencies, skills, and capacities for establishing context reflect demographic diversity and broad domain and user experience expertise, and their participation is documented. Opportunities for interdisciplinary collaboration are prioritized."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"The organization\u2019s mission and relevant goals for AI technology are understood and documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"The business value or context of business use has been clearly defined or \u2013 in the case of assessing existing AI systems \u2013 re-evaluated."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Organizational risk tolerances are determined and documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"System requirements (e.g., \u201cthe system shall respect the privacy of its users\u201d) are elicited from and understood by relevant AI actors. Design decisions take socio-technical implications into account to address AI risks."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"The specific tasks and methods used to implement the tasks that the AI system will support are defined (e.g., classifiers, generative models, recommenders)."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Information about the AI system\u2019s knowledge limits and how system output may be utilized and overseen by humans is documented. Documentation provides sufficient information to assist relevant AI actors when making decisions and taking subsequent actions."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Scientific integrity and TEVV considerations are identified and documented, including those related to experimental design, data collection and selection (e.g., availability, representativeness, suitability), system trustworthiness, and construct validation."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Potential benefits of intended AI system functionality and performance are examined and documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Potential costs, including non-monetary costs, which result from expected or realized AI errors or system functionality and trustworthiness \u2013 as connected to organizational risk tolerance \u2013 are examined and documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Targeted application scope is specified and documented based on the system\u2019s capability, established context, and AI system categorization."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Processes for operator and practitioner proficiency with AI system performance and trustworthiness \u2013 and relevant technical standards and certifications \u2013 are defined, assessed, and documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Processes for human oversight are defined, assessed, and documented in accordance with organizational policies from the GOVERN function."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Approaches for mapping AI technology and legal risks of its components \u2013 including the use of third-party data or software \u2013 are in place, followed, and documented, as are risks of infringement of a third party\u2019s intellectual property or other rights."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Internal risk controls for components of the AI system, including third-party AI technologies, are identified and documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Likelihood and magnitude of each identified impact (both potentially beneficial and harmful) based on expected use, past uses of AI systems in similar contexts, public incident reports, feedback from those external to the team that developed or deployed the AI system, or other data are identified and documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Practices and personnel for supporting regular engagement with relevant AI actors and integrating feedback about positive, negative, and unanticipated impacts are in place and documented."}
{"page":1,"tags":["Unnamed Node","Preamble","Unnamed Node"],"text":"The MEASURE function employs quantitative, qualitative, or mixed-method tools, techniques, and methodologies to analyze, assess, benchmark, and monitor AI risk and related impacts. It uses knowledge relevant to AI risks identified in the MAP function and informs the MANAGE function. AI systems should be tested before their deployment and regularly while in operation. AI risk measurements include documenting aspects of systems\u2019 functionality and trustworthiness."}
{"page":1,"tags":["Unnamed Node","Preamble","Unnamed Node"],"text":"Measuring AI risks includes tracking metrics for trustworthy characteristics, social impact, and human-AI configurations. Processes developed or adopted in the MEASURE function should include rigorous software testing and performance assessment methodologies with associated measures of uncertainty, comparisons to performance benchmarks, and formalized reporting and documentation of results. Processes for independent review can improve the effectiveness of testing and can mitigate internal biases and potential conflicts of interest."}
{"page":1,"tags":["Unnamed Node","Preamble","Unnamed Node"],"text":"Where tradeoffs among the trustworthy characteristics arise, measurement provides a traceable basis to inform management decisions. Options may include recalibration, impact mitigation, or removal of the system from design, development, production, or use, as well as a range of compensating, detective, deterrent, directive, and recovery controls."}
{"page":1,"tags":["Unnamed Node","Preamble","Unnamed Node"],"text":"After completing the MEASURE function, objective, repeatable, or scalable test, evaluation, verification, and validation (TEVV) processes including metrics, methods, and methodologies are in place, followed, and documented. Metrics and measurement methodologies should adhere to scientific, legal, and ethical norms and be carried out in an open and transparent process. New types of measurement, qualitative and quantitative, may need to be developed. The degree to which each measurement type provides unique and meaningful information to the assessment of AI risks should be considered. Framework users will enhance their capacity to comprehensively evaluate system trustworthiness, identify and track existing and emergent risks, and verify efficacy of the metrics. Measurement outcomes will be utilized in the MANAGE function to assist risk monitoring and response efforts. It is incumbent on Framework users to continue applying the MEASURE function to AI systems as knowledge, methodologies, risks, and impacts evolve over time."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Approaches and metrics for measurement of AI risks enumerated during the MAP function are selected for implementation starting with the most significant AI risks. The risks or trustworthiness characteristics that will not \u2013 or cannot \u2013 be measured are properly documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Appropriateness of AI metrics and effectiveness of existing controls are regularly assessed and updated, including reports of errors and potential impacts on affected communities."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Internal experts who did not serve as front-line developers for the system and\/or independent assessors are involved in regular assessments and updates. Domain experts, users, AI actors external to the team that developed or deployed the AI system, and affected communities are consulted in support of assessments as necessary per organizational risk tolerance."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Test sets, metrics, and details about the tools used during TEVV are documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Evaluations involving human subjects meet applicable requirements (including human subject protection) and are representative of the relevant population."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"AI system performance or assurance criteria are measured qualitatively or quantitatively and demonstrated for conditions similar to deployment setting(s). Measures are documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"The functionality and behavior of the AI system and its components \u2013 as identified in the MAP function \u2013 are monitored when in production."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"The AI system to be deployed is demonstrated to be valid and reliable. Limitations of the generalizability beyond the conditions under which the technology was developed are documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"The AI system is evaluated regularly for safety risks \u2013 as identified in the MAP function. The AI system to be deployed is demonstrated to be safe, its residual negative risk does not exceed the risk tolerance, and it can fail safely, particularly if made to operate beyond its knowledge limits. Safety metrics reflect system reliability and robustness, real-time monitoring, and response times for AI system failures."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"AI system security and resilience \u2013 as identified in the MAP function \u2013 are evaluated and documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Risks associated with transparency and accountability \u2013 as identified in the MAP function \u2013 are examined and documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"The AI model is explained, validated, and documented, and AI system output is interpreted within its context \u2013 as identified in the MAP function \u2013 to inform responsible use and governance."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Privacy risk of the AI system \u2013 as identified in the MAP function \u2013 is examined and documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Fairness and bias \u2013 as identified in the MAP function \u2013 are evaluated and results are documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Environmental impact and sustainability of AI model training and management activities \u2013 as identified in the MAP function \u2013 are assessed and documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Effectiveness of the employed TEVV metrics and processes in the MEASURE function are evaluated and documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Approaches, personnel, and documentation are in place to regularly identify and track existing, unanticipated, and emergent AI risks based on factors such as intended and actual performance in deployed contexts."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Risk tracking approaches are considered for settings where AI risks are difficult to assess using currently available measurement techniques or where metrics are not yet available."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Feedback processes for end users and impacted communities to report problems and appeal system outcomes are established and integrated into AI system evaluation metrics."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Measurement approaches for identifying AI risks are connected to deployment context(s) and informed through consultation with domain experts and other end users. Approaches are documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Measurement results regarding AI system trustworthiness in deployment context(s) and across the AI lifecycle are informed by input from domain experts and relevant AI actors to validate whether the system is performing consistently as intended. Results are documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Measurable performance improvements or declines based on consultations with relevant AI actors, including affected communities, and field data about contextrelevant risks and trustworthiness characteristics are identified and documented."}
{"page":1,"tags":["Unnamed Node","Preamble","Unnamed Node"],"text":"The MANAGE function entails allocating risk resources to mapped and measured risks on a regular basis and as defined by the GOVERN function. Risk treatment comprises plans to respond to, recover from, and communicate about incidents or events.\nContextual information gleaned from expert consultation and input from relevant AI actors \u2013 established in GOVERN and carried out in MAP \u2013 is utilized in this function to decrease the likelihood of system failures and negative impacts. Systematic documentation practices established in GOVERN and utilized in MAP and MEASURE bolster AI risk management efforts and increase transparency and accountability. Processes for assessing emergent risks are in place, along with mechanisms for continual improvement."}
{"page":1,"tags":["Unnamed Node","Preamble","Unnamed Node"],"text":"After completing the MANAGE function, plans for prioritizing risk and regular monitoring and improvement will be in place. Framework users will have enhanced capacity to manage the risks of deployed AI systems and to allocate risk management resources based on assessed and prioritized risks. It is incumbent on Framework users to continue to apply the MANAGE function to deployed AI systems as methods, contexts, risks, and needs or expectations from relevant AI actors evolve over time."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"A determination is made as to whether the AI system achieves its intended purposes and stated objectives and whether its development or deployment should proceed."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Treatment of documented AI risks is prioritized based on impact, likelihood, and available resources or methods."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Responses to the AI risks deemed high priority, as identified by the MAP function, are developed, planned, and documented. Risk response options can include mitigating, transferring, avoiding, or accepting."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Negative residual risks (defined as the sum of all unmitigated risks) to both downstream acquirers of AI systems and end users are documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Resources required to manage AI risks are taken into account \u2013 along with viable non-AI alternative systems, approaches, or methods \u2013 to reduce the magnitude or likelihood of potential impacts."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Mechanisms are in place and applied to sustain the value of deployed AI systems."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Procedures are followed to respond to and recover from a previously unknown risk when it is identified."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Mechanisms are in place and applied, and responsibilities are assigned and understood, to supersede, disengage, or deactivate AI systems that demonstrate performance or outcomes inconsistent with intended use."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"AI risks and benefits from third-party resources are regularly monitored, and risk controls are applied and documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Pre-trained models which are used for development are monitored as part of AI system regular monitoring and maintenance."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Post-deployment AI system monitoring plans are implemented, including mechanisms for capturing and evaluating input from users and other relevant AI actors, appeal and override, decommissioning, incident response, recovery, and change management."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Measurable activities for continual improvements are integrated into AI system updates and include regular engagement with interested parties, including relevant AI actors."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Incidents and errors are communicated to relevant AI actors, including affected communities. Processes for tracking, responding to, and recovering from incidents and errors are followed and documented."}
{"page":1,"tags":["Unnamed Node","Preamble","Unnamed Node"],"text":"The GOVERN function:\n\u2022\tcultivates and implements a culture of risk management within organizations designing, developing, deploying, evaluating, or acquiring AI systems;\n\u2022\toutlines processes, documents, and organizational schemes that anticipate, identify, and manage the risks a system can pose, including to users and others across society \u2013 and procedures to achieve those outcomes;\n\u2022\tincorporates processes to assess potential impacts;\n\u2022\tprovides a structure by which AI risk management functions can align with organizational principles, policies, and strategic priorities;\n\u2022\tconnects technical aspects of AI system design and development to organizational values and principles, and enables organizational practices and competencies for the individuals involved in acquiring, training, deploying, and monitoring such systems; and\n\u2022\taddresses full product lifecycle and associated processes, including legal and other issues concerning use of third-party software or hardware systems and data."}
{"page":1,"tags":["Unnamed Node","Preamble","Unnamed Node"],"text":"GOVERN is a cross-cutting function that is infused throughout AI risk management and enables the other functions of the process. Aspects of GOVERN, especially those related to compliance or evaluation, should be integrated into each of the other functions. Attention to governance is a continual and intrinsic requirement for effective AI risk management over an AI system\u2019s lifespan and the organization\u2019s hierarchy."}
{"page":1,"tags":["Unnamed Node","Preamble","Unnamed Node"],"text":"Strong governance can drive and enhance internal practices and norms to facilitate organizational risk culture. Governing authorities can determine the overarching policies that direct an organization\u2019s mission, goals, values, culture, and risk tolerance. Senior leadership sets the tone for risk management within an organization, and with it, organizational culture. Management aligns the technical aspects of AI risk management to policies and operations. Documentation can enhance transparency, improve human review processes, and bolster accountability in AI system teams."}
{"page":1,"tags":["Unnamed Node","Preamble","Unnamed Node"],"text":"After putting in place the structures, systems, processes, and teams described in the GOVERN function, organizations should benefit from a purpose-driven culture focused on risk understanding and management. It is incumbent on Framework users to continue to execute the GOVERN function as knowledge, cultures, and needs or expectations from AI actors evolve over time."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Legal and regulatory requirements involving AI are understood, managed, and documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"The characteristics of trustworthy AI are integrated into organizational policies, processes, procedures, and practices."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Processes, procedures, and practices are in place to determine the needed level of risk management activities based on the organization\u2019s risk tolerance."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"The risk management process and its outcomes are established through transparent policies, procedures, and other controls based on organizational risk priorities."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Ongoing monitoring and periodic review of the risk management process and its outcomes are planned and organizational roles and responsibilities clearly defined, including determining the frequency of periodic review."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Mechanisms are in place to inventory AI systems and are resourced according to organizational risk priorities."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Processes and procedures are in place for decommissioning and phasing out AI systems safely and in a manner that does not increase risks or decrease the organization\u2019s trustworthiness."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Roles and responsibilities and lines of communication related to mapping, measuring, and managing AI risks are documented and are clear to individuals and teams throughout the organization."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"The organization\u2019s personnel and partners receive AI risk management training to enable them to perform their duties and responsibilities consistent with related policies, procedures, and agreements."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Executive leadership of the organization takes responsibility for decisions about risks associated with AI system development and deployment."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Decision-making related to mapping, measuring, and managing AI risks throughout the lifecycle is informed by a diverse team (e.g., diversity of demographics, disciplines, experience, expertise, and backgrounds)."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Policies and procedures are in place to define and differentiate roles and responsibilities for human-AI configurations and oversight of AI systems."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Organizational policies and practices are in place to foster a critical thinking and safety-first mindset in the design, development, deployment, and uses of AI systems to minimize potential negative impacts."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Organizational teams document the risks and potential impacts of the AI technology they design, develop, deploy, evaluate, and use, and they communicate about the impacts more broadly."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Organizational practices are in place to enable AI testing, identification of incidents, and information sharing."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Organizational policies and practices are in place to collect, consider, prioritize, and integrate feedback from those external to the team that developed or deployed the AI system regarding the potential individual and societal impacts related to AI risks."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Mechanisms are established to enable the team that developed or deployed AI systems to regularly incorporate adjudicated feedback from relevant AI actors into system design and implementation."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Policies and procedures are in place that address AI risks associated with third-party entities, including risks of infringement of a third-party\u2019s intellectual property or other rights."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Contingency processes are in place to handle failures or incidents in third-party data or AI systems deemed to be high-risk."}
{"page":1,"tags":["Unnamed Node","Preamble","Unnamed Node"],"text":"The MAP function establishes the context to frame risks related to an AI system. The AI lifecycle consists of many interdependent activities involving a diverse set of actors (See Figure 3). In practice, AI actors in charge of one part of the process often do not have full visibility or control over other parts and their associated contexts. The interdependencies between these activities, and among the relevant AI actors, can make it difficult to reliably anticipate impacts of AI systems. For example, early decisions in identifying purposes and objectives of an AI system can alter its behavior and capabilities, and the dynamics of deployment setting (such as end users or impacted individuals) can shape the impacts of AI system decisions. As a result, the best intentions within one dimension of the AI lifecycle can be undermined via interactions with decisions and conditions in other, later activities.\nThis complexity and varying levels of visibility can introduce uncertainty into risk management practices. Anticipating, assessing, and otherwise addressing potential sources of negative risk can mitigate this uncertainty and enhance the integrity of the decision process."}
{"page":1,"tags":["Unnamed Node","Preamble","Unnamed Node"],"text":"The information gathered while carrying out the MAP function enables negative risk prevention and informs decisions for processes such as model management, as well as an initial decision about appropriateness or the need for an AI solution. Outcomes in the MAP function are the basis for the MEASURE and MANAGE functions. Without contextual knowledge, and awareness of risks within the identified contexts, risk management is difficult to perform. The MAP function is intended to enhance an organization\u2019s ability to identify risks and broader contributing factors."}
{"page":1,"tags":["Unnamed Node","Preamble","Unnamed Node"],"text":"Implementation of this function is enhanced by incorporating perspectives from a diverse internal team and engagement with those external to the team that developed or deployed the AI system. Engagement with external collaborators, end users, potentially impacted communities, and others may vary based on the risk level of a particular AI system, the makeup of the internal team, and organizational policies. Gathering such broad perspectives can help organizations proactively prevent negative risks and develop more trustworthy AI systems by:\n\u2022\timproving their capacity for understanding contexts;\n\u2022\tchecking their assumptions about context of use;\n\u2022\tenabling recognition of when systems are not functional within or out of their intended context;\n\u2022\tidentifying positive and beneficial uses of their existing AI systems;\n\u2022\timproving understanding of limitations in AI and ML processes;\n\u2022\tidentifying constraints in real-world applications that may lead to negative impacts;\n\u2022\tidentifying known and foreseeable negative impacts related to intended use of AI systems; and\n\u2022\tanticipating risks of the use of AI systems beyond intended use."}
{"page":1,"tags":["Unnamed Node","Preamble","Unnamed Node"],"text":"After completing the MAP function, Framework users should have sufficient contextual knowledge about AI system impacts to inform an initial go\/no-go decision about whether to design, develop, or deploy an AI system. If a decision is made to proceed, organizations should utilize the MEASURE and MANAGE functions along with policies and procedures put into place in the GOVERN function to assist in AI risk management efforts. It is incumbent on Framework users to continue applying the MAP function to AI systems as context, capabilities, risks, benefits, and potential impacts evolve over time."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Intended purposes, potentially beneficial uses, contextspecific laws, norms and expectations, and prospective settings in which the AI system will be deployed are understood and documented. Considerations include: the specific set or types of users along with their expectations; potential positive and negative impacts of system uses to individuals, communities, organizations, society, and the planet; assumptions and related limitations about AI system purposes, uses, and risks across the development or product AI lifecycle; and related TEVV and system metrics."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Interdisciplinary AI actors, competencies, skills, and capacities for establishing context reflect demographic diversity and broad domain and user experience expertise, and their participation is documented. Opportunities for interdisciplinary collaboration are prioritized."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"The organization\u2019s mission and relevant goals for AI technology are understood and documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"The business value or context of business use has been clearly defined or \u2013 in the case of assessing existing AI systems \u2013 re-evaluated."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Organizational risk tolerances are determined and documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"System requirements (e.g., \u201cthe system shall respect the privacy of its users\u201d) are elicited from and understood by relevant AI actors. Design decisions take socio-technical implications into account to address AI risks."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"The specific tasks and methods used to implement the tasks that the AI system will support are defined (e.g., classifiers, generative models, recommenders)."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Information about the AI system\u2019s knowledge limits and how system output may be utilized and overseen by humans is documented. Documentation provides sufficient information to assist relevant AI actors when making decisions and taking subsequent actions."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Scientific integrity and TEVV considerations are identified and documented, including those related to experimental design, data collection and selection (e.g., availability, representativeness, suitability), system trustworthiness, and construct validation."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Potential benefits of intended AI system functionality and performance are examined and documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Potential costs, including non-monetary costs, which result from expected or realized AI errors or system functionality and trustworthiness \u2013 as connected to organizational risk tolerance \u2013 are examined and documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Targeted application scope is specified and documented based on the system\u2019s capability, established context, and AI system categorization."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Processes for operator and practitioner proficiency with AI system performance and trustworthiness \u2013 and relevant technical standards and certifications \u2013 are defined, assessed, and documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Processes for human oversight are defined, assessed, and documented in accordance with organizational policies from the GOVERN function."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Approaches for mapping AI technology and legal risks of its components \u2013 including the use of third-party data or software \u2013 are in place, followed, and documented, as are risks of infringement of a third party\u2019s intellectual property or other rights."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Internal risk controls for components of the AI system, including third-party AI technologies, are identified and documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Likelihood and magnitude of each identified impact (both potentially beneficial and harmful) based on expected use, past uses of AI systems in similar contexts, public incident reports, feedback from those external to the team that developed or deployed the AI system, or other data are identified and documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Practices and personnel for supporting regular engagement with relevant AI actors and integrating feedback about positive, negative, and unanticipated impacts are in place and documented."}
{"page":1,"tags":["Unnamed Node","Preamble","Unnamed Node"],"text":"The MEASURE function employs quantitative, qualitative, or mixed-method tools, techniques, and methodologies to analyze, assess, benchmark, and monitor AI risk and related impacts. It uses knowledge relevant to AI risks identified in the MAP function and informs the MANAGE function. AI systems should be tested before their deployment and regularly while in operation. AI risk measurements include documenting aspects of systems\u2019 functionality and trustworthiness."}
{"page":1,"tags":["Unnamed Node","Preamble","Unnamed Node"],"text":"Measuring AI risks includes tracking metrics for trustworthy characteristics, social impact, and human-AI configurations. Processes developed or adopted in the MEASURE function should include rigorous software testing and performance assessment methodologies with associated measures of uncertainty, comparisons to performance benchmarks, and formalized reporting and documentation of results. Processes for independent review can improve the effectiveness of testing and can mitigate internal biases and potential conflicts of interest."}
{"page":1,"tags":["Unnamed Node","Preamble","Unnamed Node"],"text":"Where tradeoffs among the trustworthy characteristics arise, measurement provides a traceable basis to inform management decisions. Options may include recalibration, impact mitigation, or removal of the system from design, development, production, or use, as well as a range of compensating, detective, deterrent, directive, and recovery controls."}
{"page":1,"tags":["Unnamed Node","Preamble","Unnamed Node"],"text":"After completing the MEASURE function, objective, repeatable, or scalable test, evaluation, verification, and validation (TEVV) processes including metrics, methods, and methodologies are in place, followed, and documented. Metrics and measurement methodologies should adhere to scientific, legal, and ethical norms and be carried out in an open and transparent process. New types of measurement, qualitative and quantitative, may need to be developed. The degree to which each measurement type provides unique and meaningful information to the assessment of AI risks should be considered. Framework users will enhance their capacity to comprehensively evaluate system trustworthiness, identify and track existing and emergent risks, and verify efficacy of the metrics. Measurement outcomes will be utilized in the MANAGE function to assist risk monitoring and response efforts. It is incumbent on Framework users to continue applying the MEASURE function to AI systems as knowledge, methodologies, risks, and impacts evolve over time."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Approaches and metrics for measurement of AI risks enumerated during the MAP function are selected for implementation starting with the most significant AI risks. The risks or trustworthiness characteristics that will not \u2013 or cannot \u2013 be measured are properly documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Appropriateness of AI metrics and effectiveness of existing controls are regularly assessed and updated, including reports of errors and potential impacts on affected communities."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Internal experts who did not serve as front-line developers for the system and\/or independent assessors are involved in regular assessments and updates. Domain experts, users, AI actors external to the team that developed or deployed the AI system, and affected communities are consulted in support of assessments as necessary per organizational risk tolerance."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Test sets, metrics, and details about the tools used during TEVV are documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Evaluations involving human subjects meet applicable requirements (including human subject protection) and are representative of the relevant population."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"AI system performance or assurance criteria are measured qualitatively or quantitatively and demonstrated for conditions similar to deployment setting(s). Measures are documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"The functionality and behavior of the AI system and its components \u2013 as identified in the MAP function \u2013 are monitored when in production."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"The AI system to be deployed is demonstrated to be valid and reliable. Limitations of the generalizability beyond the conditions under which the technology was developed are documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"The AI system is evaluated regularly for safety risks \u2013 as identified in the MAP function. The AI system to be deployed is demonstrated to be safe, its residual negative risk does not exceed the risk tolerance, and it can fail safely, particularly if made to operate beyond its knowledge limits. Safety metrics reflect system reliability and robustness, real-time monitoring, and response times for AI system failures."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"AI system security and resilience \u2013 as identified in the MAP function \u2013 are evaluated and documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Risks associated with transparency and accountability \u2013 as identified in the MAP function \u2013 are examined and documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"The AI model is explained, validated, and documented, and AI system output is interpreted within its context \u2013 as identified in the MAP function \u2013 to inform responsible use and governance."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Privacy risk of the AI system \u2013 as identified in the MAP function \u2013 is examined and documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Fairness and bias \u2013 as identified in the MAP function \u2013 are evaluated and results are documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Environmental impact and sustainability of AI model training and management activities \u2013 as identified in the MAP function \u2013 are assessed and documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Effectiveness of the employed TEVV metrics and processes in the MEASURE function are evaluated and documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Approaches, personnel, and documentation are in place to regularly identify and track existing, unanticipated, and emergent AI risks based on factors such as intended and actual performance in deployed contexts."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Risk tracking approaches are considered for settings where AI risks are difficult to assess using currently available measurement techniques or where metrics are not yet available."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Feedback processes for end users and impacted communities to report problems and appeal system outcomes are established and integrated into AI system evaluation metrics."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Measurement approaches for identifying AI risks are connected to deployment context(s) and informed through consultation with domain experts and other end users. Approaches are documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Measurement results regarding AI system trustworthiness in deployment context(s) and across the AI lifecycle are informed by input from domain experts and relevant AI actors to validate whether the system is performing consistently as intended. Results are documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Measurable performance improvements or declines based on consultations with relevant AI actors, including affected communities, and field data about contextrelevant risks and trustworthiness characteristics are identified and documented."}
{"page":1,"tags":["Unnamed Node","Preamble","Unnamed Node"],"text":"The MANAGE function entails allocating risk resources to mapped and measured risks on a regular basis and as defined by the GOVERN function. Risk treatment comprises plans to respond to, recover from, and communicate about incidents or events.\nContextual information gleaned from expert consultation and input from relevant AI actors \u2013 established in GOVERN and carried out in MAP \u2013 is utilized in this function to decrease the likelihood of system failures and negative impacts. Systematic documentation practices established in GOVERN and utilized in MAP and MEASURE bolster AI risk management efforts and increase transparency and accountability. Processes for assessing emergent risks are in place, along with mechanisms for continual improvement."}
{"page":1,"tags":["Unnamed Node","Preamble","Unnamed Node"],"text":"After completing the MANAGE function, plans for prioritizing risk and regular monitoring and improvement will be in place. Framework users will have enhanced capacity to manage the risks of deployed AI systems and to allocate risk management resources based on assessed and prioritized risks. It is incumbent on Framework users to continue to apply the MANAGE function to deployed AI systems as methods, contexts, risks, and needs or expectations from relevant AI actors evolve over time."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"A determination is made as to whether the AI system achieves its intended purposes and stated objectives and whether its development or deployment should proceed."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Treatment of documented AI risks is prioritized based on impact, likelihood, and available resources or methods."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Responses to the AI risks deemed high priority, as identified by the MAP function, are developed, planned, and documented. Risk response options can include mitigating, transferring, avoiding, or accepting."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Negative residual risks (defined as the sum of all unmitigated risks) to both downstream acquirers of AI systems and end users are documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Resources required to manage AI risks are taken into account \u2013 along with viable non-AI alternative systems, approaches, or methods \u2013 to reduce the magnitude or likelihood of potential impacts."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Mechanisms are in place and applied to sustain the value of deployed AI systems."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Procedures are followed to respond to and recover from a previously unknown risk when it is identified."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Mechanisms are in place and applied, and responsibilities are assigned and understood, to supersede, disengage, or deactivate AI systems that demonstrate performance or outcomes inconsistent with intended use."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"AI risks and benefits from third-party resources are regularly monitored, and risk controls are applied and documented."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Pre-trained models which are used for development are monitored as part of AI system regular monitoring and maintenance."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Post-deployment AI system monitoring plans are implemented, including mechanisms for capturing and evaluating input from users and other relevant AI actors, appeal and override, decommissioning, incident response, recovery, and change management."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Measurable activities for continual improvements are integrated into AI system updates and include regular engagement with interested parties, including relevant AI actors."}
{"page":1,"tags":["Unnamed Node","Unnamed Node","Unnamed Node"],"text":"Incidents and errors are communicated to relevant AI actors, including affected communities. Processes for tracking, responding to, and recovering from incidents and errors are followed and documented."}
